## A Prompting Mini Lab

### Objective: 
* Understand the difference between Base, Instruction-tuned, and Reasoning language models. 
* Gain familiarity with HuggingFace

## Deliverable
A document or spreadsheet with your observations. This doesn't have to be detailed -- just a table with the model name, the prompt, the output, and your qualtiative feedback. If you would rather write your impressions, a paragraph with all the same information would also be fine.

## Steps
[video of how to navigage HF](https://gc-cuny-edu.zoom.us/rec/share/NWkuuCoeJS0B4YYzlYtWZUFjxDEv1VFD1-lkQHlEhF9dyuoqErX24nJSUNsbBHP0.WFbwphmvzfK4aeHv?startTime=1770691807000)
Passcode: Sx#%3$1y

Note the video only includes up to step 6

1. Set up an account with and [Log in to huggingface](https://huggingface.co/)
2. Search for the llama models
3. Select a base (not Instruct) model
4. Prompt it with an instruction
5. Prompt it with the start of a sentence
6. Select an Instruction-tuned one and prompt it with the same prompt pair.
7. Feel free to check for bias (i.e., with gendered names, occupations, etc.)
8. Now find a Reasoning Model [this is a good option](https://huggingface.co/zai-org/GLM-4.7-Flash?inference_provider=novita)
9. Prompt it with the same instruction you used above. 
11. When it finishes generating a response, there should be a box at the top of the response that says 'Reasoning'. Open that
12. Inspect the reasoning traces. What do you notice? Really read through all of the steps - how did it break down this task? Is it how you would have broken it down?
13. Add the model, prompt, result, and a new column for reasoning traces. 
14. What happens if you give it a different task? 






